{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Electric Vehicle Charging on Residential Power Grid\n",
    "\n",
    "## Objective:\n",
    "\n",
    "The city of Ipswich, MA uses electric meters which records usage patterns every hour in kilowatt hours (kWh).  Determine which residences are likely to be charging electric vehicles (EVs) based on usage patterns.  This data could be used to modify behavior through incentives to shift charging to off-peak times and reduce peak demand on the grid.\n",
    "\n",
    "## Data:\n",
    "\n",
    "- Initially 23 electric meters will be analyzed.  This could eventually be expanded up to identify EVs for the approximately 7000 residential meters.\n",
    "- The original 23 meters were chosen because some of them are known to have EVs.  These will not be revealed until after the data is summarized.  Other houses in the list do not have confirmed EVs, but they have also not been confirmed to not have an EV.\n",
    "- The data is in CSV files, where the file name is the meter number.  \n",
    "- Each CSV file has at minimum the following columns: \n",
    "    - \"Reading Date\"\n",
    "    - \"Consumption (kWh)\"\n",
    "- Houses with solar panels will also have:\n",
    "    - \"Energy Received (kWh)\"\n",
    "- Note: Meter IDs have been modified for this project.\n",
    "\n",
    "## Methodology:\n",
    "\n",
    "- The accompanying PowerPoint slides discuss the trigger criteria, but the following key indicators were determined:\n",
    "    - Count of usage which is 4.5 kWh higher than the average usage 2 hours prior and 2 hours after each record examined.\n",
    "        - This is likely to count a high power (24 amp, 5.7 kW) EV charger while ignoring things like electric stoves, ovens, and dryers.\n",
    "    - Count of usage which is at least 20 kWh over 5 hours.\n",
    "        - This will catch lower power chargers based on a large usage over a long duration.\n",
    "    - Total-off peak (between 12am and 7am)\n",
    "        - This is intended to identity users with high overnight charging as a catch all for everything else when there is less noise in the data.  High ranking houses in this category which are not in the other two categories may be considered as potential EV houses.\n",
    "        \n",
    "    \n",
    "\n",
    "- Focus on 2019 rather than 2020: Due to pandemic, EVs may not charge regularly if owner is working from home.\n",
    "- Ignore summer months: Air conditioning will skew results\n",
    "- Ignore winter months: Electric heat and heat pumps may skew results\n",
    "\n",
    "Therefore, data will summarize April, May, September, and October of 2019\n",
    "\n",
    "It is expected that there will be:\n",
    "- False positives caused by other high energy uses\n",
    "- False negatives caused by short duration charging, an EV with a low capacity battery (example plug in hybrids), using work or other chargers, or a low power charger.\n",
    "- The intent is to try to be more inclusive with the final summary since suspected EV houses can be confirmed once the filtered list of houses is generated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get List of File Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Meter Data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4256/2123357208.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#create list of file names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdir_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Meter Data/'\u001b[0m \u001b[1;31m#reversed slashes to make path valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfilenamelist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfilenamelist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Meter Data/'"
     ]
    }
   ],
   "source": [
    "#create list of file names\n",
    "dir_name='Meter Data/' #reversed slashes to make path valid\n",
    "filenamelist = os.listdir(dir_name) \n",
    "filenamelist\n",
    "\n",
    "#merge file name and file path\n",
    "pathlist=[]\n",
    "for filename in filenamelist:\n",
    "    pathlist.append(os.path.join(dir_name, filename))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Prevent Division by Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to prevent division by zero when finding average.\n",
    "def safe_div(n,d):\n",
    "    if d == 0:\n",
    "        return 0\n",
    "    return (n/d).round(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data\n",
    "\n",
    "#### Function: function_one_hour\n",
    "- Compare each record to the average of 2 hours back and 2 hours forward.  Trigger if current record is 4.5 kWh greater than before and after average.\n",
    "\n",
    "- Return:\n",
    "    - Count of 1 hour triggers\n",
    "    - Average value that caused trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_one_hour(df): \n",
    "    df['TriggerTest']=df[\"Consumption (kWh)\"]-(df['Consumption (kWh)'].shift(2)+df['Consumption (kWh)'].shift(-2))/2\n",
    "    \n",
    "    df['One_Hour_Trigger']=df.apply(lambda x: 1 if x['TriggerTest'] >= 4.5 else 0, axis=1)\n",
    "      \n",
    "    mask_one_hr=((df['One_Hour_Trigger']-df['One_Hour_Trigger'].shift(1)) == 1)\n",
    "    df=df.loc[mask_one_hr]\n",
    "\n",
    "    trigger_avg=safe_div(df['TriggerTest'].sum(),df['TriggerTest'].count()) #take average using safe_div function to avoid dividing by zero\n",
    "    trigger_count=df['One_Hour_Trigger'].count() \n",
    "    return trigger_count, trigger_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: function_five_hour\n",
    "- Find sections of data with a usage equal to or over 20 kW for 5 hours which is an average of at least 4kWh\n",
    "- Return:\n",
    "    - Count of 5 hour triggers \n",
    "    - Average value that caused the trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_five_hour(df):\n",
    "    df['Five_Hour_Sum']=df[\"Consumption (kWh)\"]+df['Consumption (kWh)'].shift(2)+df['Consumption (kWh)'].shift(1)+df['Consumption (kWh)'].shift(-1)+df['Consumption (kWh)'].shift(-2)\n",
    "    #display(df)\n",
    "    df['Five_Hour_Trigger']=df.apply(lambda x: 1 if x['Five_Hour_Sum'] >= 20 else 0, axis=1)\n",
    "    #display(df)\n",
    "    mask_five_hr=((df['Five_Hour_Trigger']-df['Five_Hour_Trigger'].shift(1)) == 1)\n",
    "    df=df.loc[mask_five_hr]\n",
    "    #display(df)\n",
    "    five_hour_trigger_avg=safe_div(df['Five_Hour_Sum'].sum(),df['Five_Hour_Sum'].count()) #take average using safe_div function to avoid dividing by zero\n",
    "    five_hour_trigger_count=df['Five_Hour_Trigger'].count() \n",
    "    return five_hour_trigger_count, five_hour_trigger_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: function_night_usage\n",
    "\n",
    "- Sum up usage data between 12AM and 6AM\n",
    "- Find max peak usage between 12AM and 6AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_night_usage(df):\n",
    "    df=df.sort_values(by='Reading Date')\n",
    "    df=df.set_index('Reading Date')\n",
    "    df=df.between_time('01:00', '06:00') #shifted forward an hour since data is recorderded at the start of following hour\n",
    "    off_peak_usage=int((df['Consumption (kWh)']).sum())\n",
    "    off_peak_usage_max=int((df['Consumption (kWh)']).max())\n",
    "    return off_peak_usage, off_peak_usage_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: getvalues\n",
    "\n",
    "- Determine if house has solar panels\n",
    "- Filter by 2019\n",
    "    - Collect usage summary for 2019\n",
    "- Filter by April, May, September, and October\n",
    "    - Collect usage summary for spring and fall\n",
    "    - Collect 1hr and 5hr triggers\n",
    "    - Collect off-peak summary data\n",
    "- Append dataframe with all summary values for specific meter file being read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finallist=[]\n",
    "dfworking=[]\n",
    "j = 0\n",
    "def getvalues(filename, year):\n",
    "    global j\n",
    "    dfworking=pd.read_csv(pathlist[j]) #read meter to be analyized\n",
    "    j=j+1\n",
    "    \n",
    "    dfworking['Reading Date']=pd.to_datetime(dfworking['Reading Date']) #ensure \"Reading Date\" column is in date format\n",
    "    meter = filename.replace('.csv', \"\", 1) #remove .csv suffix\n",
    "    \n",
    "    #Find houses with solar panels by looking for \"Energy Received (kWh)\" column\n",
    "    if 'Energy Received (kWh)' in dfworking.columns: \n",
    "        if (dfworking['Energy Received (kWh)']).count() > 0:\n",
    "            solar_panels = True\n",
    "        else: solar_panels = False\n",
    "    else: solar_panels = False\n",
    "    \n",
    "    #Create mask to filter by year\n",
    "    maskinitial=((dfworking['Reading Date'] >= year+'-01-01 00:00:00') & (dfworking['Reading Date'] < str(int(year)+1)+'-01-01 00:00:00'))\n",
    "    dfworking=dfworking.loc[maskinitial]     \n",
    "    null_values_2019=int(dfworking['Consumption (kWh)'].isnull().sum())\n",
    "    dfworking = dfworking.dropna()    \n",
    "   \n",
    "    #if dfworking has records\n",
    "    if len(dfworking.index) > 0:    \n",
    "       \n",
    "        #Get max and min dates in 2019\n",
    "        date_max=max(dfworking['Reading Date'])\n",
    "        date_min=min(dfworking['Reading Date'])\n",
    "        consumption_max_2019=int(dfworking[\"Consumption (kWh)\"].max())\n",
    "\n",
    "        #filter consumption over 50kWh so it data in error doesn't skew average\n",
    "        maskhigh=dfworking['Consumption (kWh)'] <50\n",
    "        dfworking=dfworking.loc[maskhigh]\n",
    "        consumption_sum_2019=int(dfworking[\"Consumption (kWh)\"].sum())\n",
    "\n",
    "        #Mask to only include: Apr, May, Sep, Oct\n",
    "        mask=(((dfworking['Reading Date'] >= year+'-04-01 00:00:00') & (dfworking['Reading Date'] < year+'-06-01 00:00:00')) |\n",
    "        ((dfworking['Reading Date'] >= year+'-09-01 00:00:00') & (dfworking['Reading Date'] < year+'-11-01 00:00:00'))) #create mask by desired date range\n",
    "        dfworking=dfworking.loc[mask]\n",
    "\n",
    "        #if dfworking has records\n",
    "        if len(dfworking.index) > 0:   \n",
    "            \n",
    "            #summarize data\n",
    "            consumption_max=int(dfworking[\"Consumption (kWh)\"].max())\n",
    "            consumption_sum=int(dfworking[\"Consumption (kWh)\"].sum())\n",
    "            trigger_count, trigger_avg=function_one_hour(dfworking)\n",
    "            five_hour_trigger_count, five_hour_trigger_avg=function_five_hour(dfworking)\n",
    "            off_peak_usage, off_peak_usage_max =function_night_usage(dfworking) \n",
    "        else:\n",
    "            consumption_max=0\n",
    "            consumption_sum=0\n",
    "            trigger_avg=0\n",
    "            trigger_count=0\n",
    "            five_hour_trigger_count=0\n",
    "            five_hour_trigger_avg=0\n",
    "            off_peak_usage=0\n",
    "            off_peak_usage_max=0\n",
    "    else:\n",
    "        date_max=0\n",
    "        date_min=0\n",
    "        consumption_max_2019=0\n",
    "        consumption_sum_2019=0\n",
    "        consumption_max=0\n",
    "        consumption_sum=0\n",
    "        trigger_avg=0\n",
    "        trigger_count=0\n",
    "        five_hour_trigger_count=0\n",
    "        five_hour_trigger_avg=0\n",
    "        off_peak_usage=0\n",
    "        off_peak_usage_max=0\n",
    "\n",
    "    #print status of script\n",
    "    print(j,' of ', len(filenamelist),'  |   Meter: ', meter, '|Solar: ', solar_panels,'|  1hr Trigger Count: ', trigger_count, '|  5hr Trigger Count: ', five_hour_trigger_count, '|  Consumption Sum: ', consumption_sum)\n",
    "    \n",
    "    #append summary of each file to \"finallist\"\n",
    "    finallist.append([meter, solar_panels, trigger_count, trigger_avg, five_hour_trigger_count, five_hour_trigger_avg, off_peak_usage, off_peak_usage_max, consumption_max, consumption_sum, consumption_sum_2019, consumption_max_2019, null_values_2019, date_max, date_min])    \n",
    "    \n",
    "    return(finallist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through all data files available.  Use getvalues function to combine all summary data into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define year to be analyized\n",
    "year='2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in filenamelist:\n",
    "    getvalues(i, year)\n",
    "\n",
    "#rename columns and write to new dataframe \"dfsummary\"\n",
    "dfsummary=[]\n",
    "dfsummary=pd.DataFrame(finallist, columns=[\"Meter Number\", \"Solar Panels?\", \"1hr Trigger Count\", \"1hr Trigger Average\", '5hr Trigger Count', '5hr Trigger Avg', 'Offpeak Total (kwh)', 'Offpeak Max (kwh)', 'Consumption Max (kWh)', 'Consumption Sum (kWh)', 'Consumption Sum 2019 (kWh)','Consumption Max 2019 (kWh)', 'Missing Data 2019 (Hours)', 'Max Date 2019', 'Min Date 2019'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort final dataframe and write to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sort data\n",
    "dfsummary=dfsummary.sort_values(by=['1hr Trigger Count','5hr Trigger Count','Offpeak Total (kwh)'], ascending=False)\n",
    "dfsummary=dfsummary.set_index('Meter Number')\n",
    "display(dfsummary)\n",
    "\n",
    "#write data to CSV file\n",
    "dfsummary.to_csv('FinalSummary'+year+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "After summarizing and reviewing data as well as adding in the known EVs, the data was color coded by an apparent drop off in the number of triggers.  While this part is open to interpretation, the idea was to look for a clear drop-off which could be adjusted if necessary, to control how many false positives were acceptable.\n",
    "The initial criteria are:\n",
    "- 6x or higher 1 hour triggers\n",
    "- 6x or higher 5 hour triggers\n",
    "- 500 kWh total usage & at least 4 kWh Max Peak during off-peak hours\n",
    "\n",
    "If a house has at least 2 of the above criteria it is consider a high probability EV house.\n",
    "If it only has 1 of the criteria it is considered a low probability EV house.\n",
    "\n",
    "#### Below is the summary table after manipulating in Excel to add extra information and to color code\n",
    "<img src=\"Town Data Final Summary.png\" />\n",
    "\n",
    "Based on the above criteria the final tally is below.  Keep in mind that the criteria could be adjusted after knowing which houses have an EV.  The idea is there is likely always going to be some false positives and false negatives, but the criteria must be selected to minimize them.\n",
    "\n",
    "- 12 houses were categorized as to not have an EV\n",
    "- 5 houses were categorized to have an EV and have a confirmed EV.\n",
    "- 4 houses were false positives, however upon visual inspection all 4 off them had some EV charging qualities, so they are considered good false positives to include in the count since they may turn out to have EVs.\n",
    "- 2 houses with false negatives, however upon visual inspection both of these did not show EV charging qualities.  These owners may have an EV with a small battery or utilize off site charging.\n",
    "\n",
    "#### Overall this trial run of the data was deemed successful and is ready to expand to a much larger dataset to identify EV owners and reach out to them for potential rate plan incentives to get them to not charge during periods of peak usage.  \n",
    "\n",
    "Future improvements:\n",
    "- The color coding of high and medium probability was done by hand since it was unclear what a good cutoff point would be.  On a larger dataset the rules should be programed in to automatically identify each group based on their \"trigger count\".\n",
    "- Have a larger dataset of confirmed houses with and without EVs.  Where not all the houses had this information confirmed, it was difficult to make rules based on assumptions.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Addendum\n",
    "#### K Means Clustering (Unsupervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define year to be analyized\n",
    "year='2019'\n",
    "\n",
    "#Read summary table which allows allows continuation without reading each individual file.\n",
    "dfsummary=pd.read_csv('FinalSummary'+year+'.csv',index_col=0)\n",
    "\n",
    "dfsummary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the table to just the categories we want to use for kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouping=dfsummary[['Solar Panels?', '1hr Trigger Count', '5hr Trigger Count', 'Offpeak Total (kwh)', 'Offpeak Max (kwh)']]\n",
    "X=df_grouping\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create numpy array with customers known to have EVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_answers=['NO', 'EV', 'EV', 'NO', 'EV', 'NO', 'NO', 'EV', 'NO', 'NO', 'NO', 'EV', 'NO', 'NO', 'NO', 'EV', 'EV', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO']\n",
    "y = pd.DataFrame({'EV?': ev_answers}).to_numpy().ravel() # convert to 1D numpy array\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use elbow method to explore values for K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(X)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values from 3 and 5 seem of most interest.  Let's use 4.\n",
    "Create 4 clusters and create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterNum = 4\n",
    "k_means = KMeans(init = \"k-means++\", n_clusters = clusterNum, n_init = 12)\n",
    "k_means.fit(X)\n",
    "labels = k_means.labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign cluster labels to df_grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_grouping_clusters=df_grouping\n",
    "df_grouping_clusters.loc[:,\"Clusters\"] = labels\n",
    "df_grouping_clusters.loc[:,\"EV?\"] = ev_answers\n",
    "df_grouping_clusters.loc[:,'EV?'] = df_grouping_clusters['EV?'].map({'EV': 1, 'NO': 0})\n",
    "df_grouping_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Group by Cluster Number and Average Columns')\n",
    "display(df_grouping_clusters.groupby('Clusters').mean())\n",
    "group_summary=df_grouping_clusters.groupby('Clusters').agg({'EV?': ['sum', 'count']})\n",
    "print('Group by Cluster Number and Sum EVs to compare against total count')\n",
    "display(group_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shows thats cluster 1 contains most of the EVs with 5 out of 6 (83%).  The only meter which is incorrectly classified is meter R85461, but it does seem to fit the group based on the summary results.\n",
    "\n",
    "Cluster 2 and 3 correctly show no EVs.  However, if just looking at the summary you'd be tempted to put the single car in cluster 2 in the EV category since it has the most triggers.\n",
    "\n",
    "Cluster 0 shows 2 EVs out of 9 customers (22%).  Upon looking at the summary data these were two cars which have seemed to be outliers in the data.  Meter R48976 is a BMW EV which has a small battery, so it may not show up in the electric usage data and meter R47897 could not be confirmed to have an EV, but was believed to be a Volt.\n",
    "\n",
    "Overall this method of K Means Grouping seems to have provided a reasonable result and could provide a good result when scaling to larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's repeat the process with K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Learn and Test Set  \n",
    "This data set is probably a bit small, but lets explore to see what results we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine Best K value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = 10\n",
    "mean_acc = np.zeros((Ks-1))\n",
    "std_acc = np.zeros((Ks-1))\n",
    "\n",
    "for n in range(1,Ks):\n",
    "    \n",
    "    #Train Model and Predict  \n",
    "    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n",
    "    yhat=neigh.predict(X_test)\n",
    "    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n",
    "\n",
    "    \n",
    "    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
    "\n",
    "mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all K values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,Ks),mean_acc,'g')\n",
    "plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\n",
    "plt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\n",
    "plt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.xlabel('Number of Neighbors (K)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is small lets use cross validation to use all the data.\n",
    "\n",
    "Create KNN model.  From the previous test with the limited dataset, lets define the k value (n_neighbors at 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cross validation, lets break the data up into 5 sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(knn_cv, X, y, cv=5)\n",
    "#print each cv score (accuracy) and average them\n",
    "print('Score for each of the 5 sets: ',cv_scores)\n",
    "print('cv_scores mean: ', np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the K Nearest Neighbor method produced an average of 78% accuracy.\n",
    "Future recommendations:\n",
    "- Increase the training and test size\n",
    "- Get further confirmation on if the meters which don't show an EV truely do not have one.\n",
    "- Deploy method onto full dataset of 7000 meters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
